{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "cd_path = '../../chromedriver'\n",
    "driver = webdriver.Chrome(cd_path)\n",
    "url = \"https://www.youtube.com\"\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "keyword = \"(G)I-DLE (여자)아이들\"\n",
    "driver.find_element_by_css_selector(\"#search\").click()\n",
    "driver.find_element_by_css_selector(\"#search\").send_keys(keyword)\n",
    "driver.find_element_by_css_selector(\"#search\").send_keys(Keys.RETURN)\n",
    "\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "driver.find_element_by_xpath(\"//div[@id='info']/ytd-channel-name[@id='channel-title']\").click()\n",
    "\n",
    "driver.find_element_by_xpath(\"//*[@id='tabsContent']/paper-tab[2]/div\").click()\n",
    "\n",
    "\n",
    "\n",
    "body = driver.find_element_by_tag_name('body')\n",
    "\n",
    "num_of_pagedown = 30\n",
    "while num_of_pagedown :\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.1)\n",
    "    num_of_pagedown -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "cd_path = '../../chromedriver'\n",
    "driver = webdriver.Chrome(cd_path)\n",
    "url = \"https://www.youtube.com\"\n",
    "driver.implicitly_wait(5)\n",
    "driver.get(url)\n",
    "\n",
    "keyword = \"(G)I-DLE (여자)아이들\"\n",
    "driver.find_element_by_css_selector(\"#PM_ID_themelist > ul > li:nth-child(10) > a\").click()\n",
    "driver.find_element_by_css_selector(\"#search\").send_keys(keyword)\n",
    "driver.find_element_by_css_selector(\"#search\").send_keys(Keys.RETURN)\n",
    "\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "driver.find_element_by_xpath(\"//div[@id='info']/ytd-channel-name[@id='channel-title']\").click()\n",
    "\n",
    "driver.find_element_by_xpath(\"//*[@id='tabsContent']/paper-tab[2]/div\").click()\n",
    "\n",
    "\n",
    "\n",
    "body = driver.find_element_by_tag_name('body')\n",
    "\n",
    "num_of_pagedown = 30\n",
    "while num_of_pagedown :\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.1)\n",
    "    num_of_pagedown -= 1\n",
    "    try : \n",
    "        driver.find_element_by_xpath(\"//*[@id='view-all']/a\").click()\n",
    "        break\n",
    "    except : \n",
    "        pass\n",
    "\n",
    "num_of_pagedown = 30\n",
    "while num_of_pagedown :\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.1)\n",
    "    num_of_pagedown -= 1\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "titles= soup.select('#PM_ID_themecastBody > div > div > div > ul > li.tl_bundle > ul > li:nth-child(1) > a')\n",
    "title.text\n",
    "views = soup.select(\"ytd-grid-renderer > div > ytd-grid-video-renderer > div > div > div > div > div > div > span\")\n",
    "for i, title in enumerate(titles) :\n",
    "    print(title.text, views[i*2].text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 날씨가 궁금한 동네의 이름을 입력해주세요 : 목동\n",
      "현재 '목동'은 18도 입니다.\n"
     ]
    }
   ],
   "source": [
    "from urllib import request, parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "town = input(\"현재 날씨가 궁금한 동네의 이름을 입력해주세요 : \")\n",
    "town_weather = parse.quote(town + \"+날씨\")\n",
    "\n",
    "url = 'https://search.naver.com/search.naver?ie=utf8&query='+ town_weather\n",
    "\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "print('현재 \\'' + town + '\\'은 ' + soup.find('p', class_='info_temperature').find('span', class_='todaytemp').text + '도 입니다.'\n",
    "      , sep = '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "cd_path = '../../chromedriver'\n",
    "driver = webdriver.Chrome(cd_path)\n",
    "url = \"https://www.youtube.com\"\n",
    "driver.implicitly_wait(5)\n",
    "driver.get(url)\n",
    "\n",
    "keyword = \"(G)I-DLE (여자)아이들\"\n",
    "driver.find_element_by_css_selector(\"#search\").click()\n",
    "driver.find_element_by_css_selector(\"#search\").send_keys(keyword)\n",
    "driver.find_element_by_css_selector(\"#search\").send_keys(Keys.RETURN)\n",
    "\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "driver.find_element_by_xpath(\"//div[@id='info']/ytd-channel-name[@id='channel-title']\").click()\n",
    "\n",
    "driver.find_element_by_xpath(\"//*[@id='tabsContent']/paper-tab[2]/div\").click()\n",
    "body = driver.find_element_by_tag_name('body')\n",
    "\n",
    "num_of_pagedown = 30\n",
    "while num_of_pagedown :\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.1)\n",
    "    num_of_pagedown -= 1\n",
    "    try : \n",
    "        driver.find_element_by_xpath(\"//*[@id='view-all']/a\").click()\n",
    "        break\n",
    "    except : \n",
    "        pass\n",
    "\n",
    "num_of_pagedown = 30\n",
    "while num_of_pagedown :\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.1)\n",
    "    num_of_pagedown -= 1\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "titles= soup.select(\"ytd-grid-renderer > div > ytd-grid-video-renderer > div > div > div > h3\")\n",
    "views = soup.select(\"ytd-grid-renderer > div > ytd-grid-video-renderer > div > div > div > div > div > div > span\")\n",
    "for i, title in enumerate(titles) :\n",
    "    print(title.text, views[i*2].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "cd_path = '../../chromedriver'\n",
    "driver = webdriver.Chrome(cd_path)\n",
    "url = \"https://www.youtube.com/results?search_query=%28G%29I-DLE+%28%EC%97%AC%EC%9E%90%29%EC%95%84%EC%9D%B4%EB%93%A4\"\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(3)\n",
    "time.sleep(3)\n",
    "\n",
    "driver.find_element_by_xpath(\"//div[@id='info']/ytd-channel-name[@id='channel-title']\").click()\n",
    "#driver.find_element_by_css_selector(\"div#info > ytd-channel-name#channel-title\").click()\n",
    "\n",
    "driver.find_element_by_xpath(\"//*[@id='tabsContent']/paper-tab[2]/div\").click()\n",
    "#driver.find_element_by_css_selector(\"#tabsContent > paper-tab:nth-child(4) > div\").click()\n",
    "\n",
    "body = driver.find_element_by_tag_name('body')\n",
    "\n",
    "num_of_pagedown = 30\n",
    "while num_of_pagedown :\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.1)\n",
    "    num_of_pagedown -= 1\n",
    "    try : \n",
    "        driver.find_element_by_xpath(\"//*[@id='view-all']/a\").click()\n",
    "        break\n",
    "    except : \n",
    "        pass\n",
    "\n",
    "num_of_pagedown = 30\n",
    "while num_of_pagedown :\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.1)\n",
    "    num_of_pagedown -= 1\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "titles= soup.select(\"ytd-grid-renderer > div > ytd-grid-video-renderer > div > div > div > h3\")\n",
    "views = soup.select(\"ytd-grid-renderer > div > ytd-grid-video-renderer > div > div > div > div > div > div > span\")\n",
    "for i, title in enumerate(titles) :\n",
    "    print(title.text, views[i*2].text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "CD_PATH = \"../../chromedriver\" #크롬 드라이버 위치\n",
    "URL = \"https://www.naver.com/\" #실행시킬 페이지 URL\n",
    "driver = webdriver.Chrome(CD_PATH) #Chrome 드라이버 실행\n",
    "driver.get(URL) #URL 넘겨서 해당 페이지 호출\n",
    "driver.implicitly_wait(3) #암묵적으로 웹 자원을 모두 불러오기 위해 3초 딜레이\n",
    "\n",
    "#id 값으로 요소 찾기\n",
    "check = driver.find_element_by_id(\"NM_RTK_ROLLING_WRAP\")\n",
    "\n",
    "#클릭이 가능한지 확인하는 함수. 클릭 가능하면 True를 return하고, 불가능하면 False를 return.\n",
    "print(check.is_enabled()) \n",
    "\n",
    "check.click()\n",
    "\n",
    "#이슈, 이벤트, 시사, 엔터, 스포츠 \n",
    "category = [\"[1]\", \"[2]\", \"[3]\", \"[4]\", \"[5]\"]\n",
    "#관심사 수준 다섯 개 \n",
    "check_nth = [\"[3]\", \"[4]\", \"[5]\", \"[2]\", \"[1]\"]\n",
    "for i, cat in enumerate(category) :\n",
    "    issue = driver.find_element_by_xpath(\"//*[@id='NM_RTK_VIEW_filter_wrap']/li\"+cat + \"/ div / a\" + check_nth[i]).click()\n",
    "    \n",
    "#연령 선택\n",
    "age_nth = [\"[1]\", \"[2]\", \"[3]\", \"[4]\", \"[5]\", \"[6]\"]\n",
    "driver.find_element_by_xpath(\"//*[@id='NM_RKT_VIEW_filter_age_wrap']/li\" + age_nth[1]).click()\n",
    "\n",
    "#아래에 time 부분은 없어도 됨\n",
    "import time\n",
    "time.sleep(3) #클릭 시간 잠시 기다려주려고 혹시 몰라서 추가함.\n",
    "\n",
    "#설정 완료 선택\n",
    "driver.find_element_by_id(\"NM_RTK_VIEW_set_btn\").click()\n",
    "\n",
    "time.sleep(3) #3초 딜레이\n",
    "html = driver.page_source #페이지 HTML 값 가져오기\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "rank = soup.select('span.keyword') #인기 검색어가 있는 부분을 추출\n",
    "#print(rank)\n",
    "\n",
    "#for i, r in enumerate(rank) :\n",
    "#    print(i+1, \"위\", r.text)\n",
    "    \n",
    "new_rank = soup.select('li > a.link_keyword > span')\n",
    "#print(new_rank)\n",
    "for i, r in enumerate(new_rank) :\n",
    "    print(i+1, \"위\", r.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "\n",
    "cd_path = '../../chromedriver'\n",
    "days = {'월' : 'mon', '화' : 'tue', '수' : 'wed', '목' : 'thu', '금' : 'fri', '토' : 'sat', '일' : 'sun'}\n",
    "days_list = list(days.values())\n",
    "\n",
    "#방법 1. 사용자한테 요일 입력받아오기\n",
    "def select_Day() :\n",
    "    print(\"'월, 화, 수, 목, 금, 토, 일' 중에 입력\")\n",
    "    day = input()\n",
    "    day = days[day]\n",
    "    return day\n",
    "    \n",
    "#방법 2. 현재 시간이 10시 40분 이후라면 내일의 웹툰 업데이트 목록 가져오기\n",
    "def calculate_Day() :\n",
    "    t = time.localtime()\n",
    "    today = time.localtime().tm_wday\n",
    "    hour = t.tm_hour\n",
    "    if(t.tm_min >=40) :\n",
    "        hour += 1\n",
    "\n",
    "    if(hour >= 23) :\n",
    "        today += 1\n",
    "    day = days_list[today]\n",
    "    return day\n",
    "    \n",
    "#day = select_Day()\n",
    "day = calculate_Day()\n",
    "url = \"https://comic.naver.com/webtoon/weekdayList.nhn?week=\" + day\n",
    "driver = webdriver.Chrome(executable_path=cd_path)\n",
    "driver.implicitly_wait(3)\n",
    "driver.get(url) \n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "lists = soup.find('ul', {'class' : 'img_list'})\n",
    "updates = lists.select(\"ul > li > div > a\")\n",
    "\n",
    "for update in updates :\n",
    "    find_em_tag = update.find('em')\n",
    "    if(find_em_tag != None) :\n",
    "        if(find_em_tag.attrs['class'] == ['ico_updt']) :\n",
    "            print(update['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request, parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "town = input(\"현재 날씨가 궁금한 동네의 이름을 입력해주세요 : \")\n",
    "town_weather = parse.quote(town + \"+날씨\")\n",
    "\n",
    "url = 'https://search.naver.com/search.naver?ie=utf8&query='+ town_weather\n",
    "\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "print('현재 \\'' + town + '\\'은 ' + soup.find('p', class_='info_temperature')\n",
    "      .find('span', class_='todaytemp').text + '도 입니다.', sep = '')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
